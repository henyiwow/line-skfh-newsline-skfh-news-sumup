import os
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import email.utils
from urllib.parse import quote
import requests
from bs4 import BeautifulSoup
import jieba
from summa import summarizer

ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')
print("âœ… Access Token å‰ 10 ç¢¼ï¼š", ACCESS_TOKEN[:10] if ACCESS_TOKEN else "æœªè¨­å®š")

# å°ç£æ™‚å€
TW_TZ = timezone(timedelta(hours=8))
now = datetime.now(TW_TZ)
today = now.strftime("%Y-%m-%d")

# åˆ†é¡é—œéµå­—åŠæ’åºï¼ˆä½ æŒ‡å®šé †åºï¼‰
CATEGORY_KEYWORDS = {
    "æ–°å…‰é‡‘æ§": ["æ–°å…‰é‡‘", "æ–°å…‰äººå£½", "æ–°å£½", "å³æ±é€²"],
    "å°æ–°é‡‘æ§": ["å°æ–°é‡‘", "å°æ–°äººå£½", "å°æ–°å£½", "å³æ±äº®"],
    "é‡‘æ§": ["é‡‘æ§", "é‡‘èæ§è‚¡", "ä¸­ä¿¡é‡‘", "ç‰å±±é‡‘", "æ°¸è±é‡‘", "åœ‹æ³°é‡‘", "å¯Œé‚¦é‡‘", "å°ç£é‡‘"],
    "ä¿éšª": ["ä¿éšª", "å£½éšª", "å¥åº·éšª", "æ„å¤–éšª", "äººå£½"],
    "å…¶ä»–": []
}
CATEGORY_ORDER = ["æ–°å…‰é‡‘æ§", "å°æ–°é‡‘æ§", "é‡‘æ§", "ä¿éšª", "å…¶ä»–"]

EXCLUDED_KEYWORDS = ['ä¿éšªå¥—', 'é¿å­•å¥—', 'ä¿éšªå¥—ä½¿ç”¨', 'å¤ªé™½äººå£½', 'å¤§è¥¿éƒ¨äººå£½', 'ç¾åœ‹æµ·å²¸ä¿éšª']

# RSS ä¾†æº
RSS_URLS = [
    "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+æ–°å£½+OR+å³æ±é€²&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å°æ–°å£½+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=é‡‘æ§+OR+é‡‘èæ§è‚¡&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å£½éšª+OR+å¥åº·éšª+OR+æ„å¤–éšª+OR+äººå£½&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
]

def shorten_url(long_url):
    try:
        encoded_url = quote(long_url, safe='')
        api_url = f"http://tinyurl.com/api-create.php?url={encoded_url}"
        res = requests.get(api_url, timeout=5)
        if res.status_code == 200:
            return res.text.strip()
    except Exception as e:
        print("âš ï¸ çŸ­ç¶²å€å¤±æ•—ï¼š", e)
    return long_url

def classify_news(title):
    title = title.lower()
    for category, keywords in CATEGORY_KEYWORDS.items():
        if any(kw.lower() in title for kw in keywords):
            return category
    return "å…¶ä»–"

def is_taiwan_news(source_name, link):
    taiwan_sources = [
        'å·¥å•†æ™‚å ±', 'ä¸­åœ‹æ™‚å ±', 'ç¶“æ¿Ÿæ—¥å ±', 'ä¸‰ç«‹æ–°èç¶²', 'è‡ªç”±æ™‚å ±', 'è¯åˆæ–°èç¶²',
        'é¡é€±åˆŠ', 'å°ç£é›…è™', 'é‰…äº¨ç¶²', 'ä¸­æ™‚æ–°èç¶²','Ettodayæ–°èé›²',
        'å¤©ä¸‹é›œèªŒ', 'å¥‡æ‘©æ–°è', 'ã€Šç¾ä»£ä¿éšªã€‹é›œèªŒ','é è¦‹é›œèªŒ'
    ]
    if any(taiwan_source in source_name for taiwan_source in taiwan_sources) and "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±" not in source_name:
        return True
    if '.tw' in link:
        return True
    return False

def extract_text_simple(url):
    try:
        res = requests.get(url, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')

        article = soup.find('article')
        if article:
            text = article.get_text(separator=' ', strip=True)
        else:
            divs = soup.find_all('div')
            divs = sorted(divs, key=lambda d: len(d.get_text(strip=True)), reverse=True)
            for d in divs:
                if len(d.get_text(strip=True)) > 200:
                    text = d.get_text(separator=' ', strip=True)
                    break
            else:
                for script in soup(['script', 'style']):
                    script.decompose()
                text = soup.get_text(separator=' ', strip=True)

        return text[:5000]
    except Exception as e:
        print(f"âš ï¸ è®€å–å…¨æ–‡å¤±æ•—: {e}")
        return ""

def summarize_text(text, max_chars=100):
    if not text:
        return "ç„¡æ³•å–å¾—å…§å®¹æ‘˜è¦"
    try:
        words = jieba.cut(text, cut_all=False)
        text_for_sum = " ".join(words)
        summary = summarizer.summarize(text_for_sum, words=50)
        if not summary:
            summary = text[:max_chars]
    except Exception:
        summary = text[:max_chars]
    return summary.replace('\n', '').replace(' ', '')[:max_chars]

def fetch_news():
    classified_news = {cat: [] for cat in CATEGORY_KEYWORDS}
    processed_links = set()

    for rss_url in RSS_URLS:
        res = requests.get(rss_url)
        print(f"âœ… ä¾†æº: {rss_url} å›æ‡‰ç‹€æ…‹ï¼š{res.status_code}")
        if res.status_code != 200:
            continue

        root = ET.fromstring(res.content)
        items = root.findall(".//item")
        print(f"âœ… å¾ {rss_url} æŠ“åˆ° {len(items)} ç­†æ–°è")

        for item in items:
            title_elem = item.find('title')
            link_elem = item.find('link')
            pubDate_elem = item.find('pubDate')
            if title_elem is None or link_elem is None or pubDate_elem is None:
                continue

            title = title_elem.text.strip()
            link = link_elem.text.strip()
            pubDate_str = pubDate_elem.text.strip()

            if not title or title.startswith("Google ãƒ‹ãƒ¥ãƒ¼ã‚¹"):
                continue

            source_elem = item.find('source')
            source_name = source_elem.text.strip() if source_elem is not None else "æœªæ¨™ç¤º"
            pub_datetime = email.utils.parsedate_to_datetime(pubDate_str).astimezone(TW_TZ)

            if now - pub_datetime > timedelta(hours=24):
                continue
            if any(bad_kw in title for bad_kw in EXCLUDED_KEYWORDS):
                continue
            if not is_taiwan_news(source_name, link):
                continue
            if link in processed_links:
                continue
            processed_links.add(link)

            short_link = shorten_url(link)
            category = classify_news(title)

            # æŠ“å…¨æ–‡+æ‘˜è¦
            full_text = extract_text_simple(link)
            summary = summarize_text(full_text, 100)

            formatted = f"ğŸ“° {title}\næ‘˜è¦ï¼š{summary}\nğŸ”— {short_link}\nä¾†æºï¼š{source_name}\n"
            classified_news[category].append(formatted)

    return classified_news

def send_line_message(message):
    if not ACCESS_TOKEN:
        print("âš ï¸ ACCESS_TOKEN æœªè¨­å®šï¼Œç„¡æ³•ç™¼é€è¨Šæ¯")
        return

    url = 'https://api.line.me/v2/bot/message/broadcast'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {ACCESS_TOKEN}'
    }

    data = {
        "messages": [{
            "type": "text",
            "text": message
        }]
    }

    print(f"ğŸ“¤ ç™¼é€è¨Šæ¯é•·åº¦ï¼š{len(message)} å­—å…ƒ")
    res = requests.post(url, headers=headers, json=data)
    print(f"ğŸ“¤ LINE å›å‚³ç‹€æ…‹ç¢¼ï¼š{res.status_code}")
    print(f"ğŸ“¤ LINE å›å‚³å…§å®¹ï¼š{res.text}")

def main():
    news = fetch_news()
    all_messages = []
    for cat in CATEGORY_ORDER:
        items = news.get(cat, [])
        if items:
            header = f"ã€{today} æ¥­ä¼éƒ¨ ä»Šæ—¥ã€{cat}ã€‘é‡é»æ–°èæ•´ç†ã€‘ å…±{len(items)}å‰‡æ–°è\n\n"
            all_messages.append(header + "\n".join(items))

    if not all_messages:
        print("âš ï¸ ç„¡ç¬¦åˆæ¢ä»¶çš„æ–°èï¼Œä¸ç™¼é€ã€‚")
        return

    final_message = "\n\n".join(all_messages)
    if len(final_message) > 4000:
        print(f"âš ï¸ è¨Šæ¯è¶…é4000å­—ï¼Œå°‡æˆªæ–·")
        final_message = final_message[:4000]

    send_line_message(final_message)

if __name__ == "__main__":
    main()


