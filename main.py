import os
import re
import feedparser
import requests
import hashlib
import base64
from datetime import datetime, timedelta
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from summa.summarizer import summarize

# === è¨­å®šå€ ===
LINE_ACCESS_TOKEN = os.getenv("LINE_ACCESS_TOKEN")  # è¨˜å¾—åœ¨ GitHub Actions secrets è¨­å®š
RSS_FEEDS = [
    "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å£½éšª+OR+é‡‘æ§+OR+äººå£½+OR+æ–°å£½+OR+å°æ–°å£½+OR+å³æ±é€²+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+æ–°å£½+OR+å³æ±é€²&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å°æ–°å£½+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å£½éšª+OR+å¥åº·éšª+OR+æ„å¤–éšª+OR+äººå£½&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=é‡‘æ§+OR+é‡‘èæ§è‚¡&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
]
EXCLUDED_KEYWORDS = ["ä¿éšªå¥—", "éŠæˆ²", "æ—…éŠ", "é…’åº—", "é›»å½±"]
CATEGORY_KEYWORDS = {
    "æ–°å…‰é‡‘æ§": ["æ–°å…‰é‡‘æ§", "æ–°å…‰äººå£½", "æ–°å£½", "å³æ±é€²"],
    "å°æ–°é‡‘æ§": ["å°æ–°é‡‘æ§", "å°æ–°äººå£½", "å°æ–°å£½", "å³æ±äº®"],
    "é‡‘æ§": ["é‡‘æ§", "é‡‘èæ§è‚¡"],
    "ä¿éšª": ["å£½éšª", "å¥åº·éšª", "æ„å¤–éšª", "ä¿éšª", "äººå£½"],
}
CATEGORY_ORDER = ["æ–°å…‰é‡‘æ§", "å°æ–°é‡‘æ§", "é‡‘æ§", "ä¿éšª", "å…¶ä»–"]
MAX_LINE_MESSAGE_LENGTH = 5000
SUMMARY_CHAR_LIMIT = 100


# === å·¥å…·æ–¹æ³• ===
def shorten_url(url):
    try:
        response = requests.post("https://cleanuri.com/api/v1/shorten", data={"url": url})
        return response.json().get("result_url", url)
    except:
        return url


def resolve_google_news_redirect(link):
    try:
        res = requests.get(link, allow_redirects=True, timeout=5)
        return res.url
    except:
        return link


def fetch_full_article(url):
    try:
        res = requests.get(url, timeout=5)
        soup = BeautifulSoup(res.text, "html.parser")
        paragraphs = soup.find_all("p")
        return " ".join(p.get_text() for p in paragraphs)
    except:
        return ""


def summarize_text(text, limit=SUMMARY_CHAR_LIMIT):
    try:
        summary = summarize(text, ratio=0.1)
        return summary.strip()[:limit] if summary else text[:limit]
    except:
        return text[:limit]


def categorize_news(title, summary):
    combined_text = title + summary
    for category, keywords in CATEGORY_KEYWORDS.items():
        if any(kw in combined_text for kw in keywords):
            return category
    return "å…¶ä»–"


def should_exclude(title):
    return any(bad_kw in title for bad_kw in EXCLUDED_KEYWORDS)


# === ä¸»æµç¨‹ ===
def main():
    print(f"âœ… Access Token å‰ 10 ç¢¼ï¼š {LINE_ACCESS_TOKEN[:10] if LINE_ACCESS_TOKEN else 'æœªè¨­å®š'}")
    all_news = []

    for url in RSS_FEEDS:
        print(f"âœ… ä¾†æº: {url}", end=" ")
        feed = feedparser.parse(url)
        print(f"å›æ‡‰ç‹€æ…‹ï¼š{feed.get('status', 'N/A')}")
        entries = feed.entries
        print(f"âœ… å¾ {url} æŠ“åˆ° {len(entries)} ç­†æ–°è")

        for entry in entries:
            title = entry.get("title", "")
            link = entry.get("link", "")
            pub_date = entry.get("published", entry.get("updated", ""))

            if not title or not link:
                continue

            parsed_time = None
            for fmt in ("%a, %d %b %Y %H:%M:%S %Z", "%Y-%m-%dT%H:%M:%SZ"):
                try:
                    parsed_time = datetime.strptime(pub_date, fmt)
                    break
                except:
                    continue
            if not parsed_time:
                continue

            now = datetime.utcnow()
            if now - parsed_time > timedelta(hours=48):  # æ”¾å¯¬ç‚º48å°æ™‚
                print(f"â³ è·³éï¼šè¶…é 48 å°æ™‚ {title}")
                continue

            if should_exclude(title):
                print(f"ğŸš« æ’é™¤é—œéµå­—å‘½ä¸­ï¼š{title}")
                continue

            original_url = resolve_google_news_redirect(link)
            full_article = fetch_full_article(original_url)
            summary = summarize_text(full_article)

            category = categorize_news(title, summary)
            short_url = shorten_url(original_url)
            line_url = short_url.replace("https://", "line://")

            all_news.append({
                "category": category,
                "title": title,
                "summary": summary,
                "url": line_url,
            })

    if not all_news:
        print("âš ï¸ ç„¡ç¬¦åˆæ¢ä»¶çš„æ–°èï¼Œä¸ç™¼é€ã€‚")
        return

    # åˆ†é¡ã€æ’åº
    categorized = {cat: [] for cat in CATEGORY_ORDER}
    for item in all_news:
        categorized[item["category"]].append(item)

    messages = []
    today = datetime.now().strftime("%Y-%m-%d")
    for cat in CATEGORY_ORDER:
        items = categorized[cat]
        if not items:
            continue

        header = f"ã€{today} æ¥­ä¼éƒ¨ ä»Šæ—¥ã€{cat}ã€‘é‡é»æ–°èæ•´ç†ã€‘"
        lines = [header]
        for news in items:
            lines.append(f"ğŸ”¹{news['title']}\næ‘˜è¦ï¼š{news['summary']}\n{news['url']}")
        message = "\n\n".join(lines)
        messages.append(message)

    # è‡ªå‹•åˆ†æ®µ
    final_messages = []
    current = ""
    for msg in messages:
        if len(current) + len(msg) + 2 > MAX_LINE_MESSAGE_LENGTH:
            final_messages.append(current)
            current = msg
        else:
            current += "\n\n" + msg
    if current:
        final_messages.append(current)

    # å‚³é€
    headers = {
        "Authorization": f"Bearer {LINE_ACCESS_TOKEN}",
        "Content-Type": "application/json"
    }
    for msg in final_messages:
        data = {"messages": [{"type": "text", "text": msg.strip()}]}
        r = requests.post("https://api.line.me/v2/bot/message/broadcast", headers=headers, json=data)
        print(f"âœ… ç™¼é€ç‹€æ…‹ï¼š{r.status_code} - {r.text}")


if __name__ == "__main__":
    main()


