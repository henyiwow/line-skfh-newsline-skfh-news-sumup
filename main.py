import feedparser
import requests
from newspaper import Article
from datetime import datetime, timedelta
from urllib.parse import quote
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from config import LINE_NOTIFY_TOKEN

# é—œéµå­—æ’åºå„ªå…ˆé †åº
KEYWORDS_ORDER = [
    "æ–°å…‰äººå£½", "æ–°å…‰é‡‘æ§", "å°æ–°äººå£½", "å°æ–°é‡‘æ§",
    "é‡‘æ§", "äººå£½", "å£½éšª", "å¥åº·éšª", "æ„å¤–éšª"
]

# RSS æœå°‹æ ¼å¼ï¼šGoogle News + æ˜¨æ—¥æ—¥æœŸ
yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
GOOGLE_NEWS_URL = "https://news.google.com/rss/search?q={query}+after:{date}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"

def fetch_news(keyword):
    query = quote(keyword)
    url = GOOGLE_NEWS_URL.format(query=query, date=yesterday)
    print(f"\nğŸ” æ­£åœ¨æŠ“å–ï¼š{keyword}\nRSSç¶²å€: {url}")
    feed = feedparser.parse(url)
    print(f"ğŸ“„ æ‰¾åˆ° {len(feed.entries)} å‰‡æ–°è")
    return feed.entries

def get_summary_from_url(url, max_sentences=3):
    try:
        print(f"ğŸ“° å˜—è©¦æŠ“å–æ‘˜è¦: {url}")
        article = Article(url)
        article.download()
        article.parse()
        text = article.text

        if not text.strip():
            print("âš ï¸ ç„¡å…§æ–‡å¯æ‘˜è¦")
            return None

        parser = PlaintextParser.from_string(text, Tokenizer("chinese"))
        summarizer = LsaSummarizer()
        summary_sentences = summarizer(parser.document, max_sentences)
        summary = " ".join(str(s) for s in summary_sentences)
        trimmed_summary = summary.strip()[:100]
        print(f"âœ… æ‘˜è¦å®Œæˆ: {trimmed_summary}")
        return trimmed_summary
    except Exception as e:
        print(f"âŒ æ‘˜è¦å¤±æ•—: {e}")
        return None

def send_line_notify(message):
    url = "https://notify-api.line.me/api/notify"
    headers = {"Authorization": f"Bearer {LINE_NOTIFY_TOKEN}"}
    data = {"message": message}
    response = requests.post(url, headers=headers, data=data)
    print(f"\nğŸ“¬ ç™¼é€ LINE Notify ç‹€æ…‹ç¢¼ï¼š{response.status_code}")
    print(f"ğŸ“¨ å›æ‡‰å…§å®¹ï¼š{response.text}")

def main():
    seen_links = set()
    final_message = None

    for keyword in KEYWORDS_ORDER:
        entries = fetch_news(keyword)
        for entry in entries:
            if entry.link in seen_links:
                continue
            seen_links.add(entry.link)

            print(f"\nğŸ”— æ–°èæ¨™é¡Œï¼š{entry.title}")
            print(f"ğŸ”— æ–°èé€£çµï¼š{entry.link}")

            summary = get_summary_from_url(entry.link)
            if summary:
                final_message = f"ğŸ“¢ã€{keyword}ã€‘\n{summary}\nğŸ‘‰ {entry.link}"
                break
        if final_message:
            break

    if final_message:
        print(f"\nğŸš€ æº–å‚™ç™¼é€çš„ LINE è¨Šæ¯ï¼š\n{final_message}")
        send_line_notify(final_message)
    else:
        print("\nâš ï¸ ç„¡ç¬¦åˆæ¢ä»¶æ–°èï¼Œç™¼é€é è¨­è¨Šæ¯")
        send_line_notify("ğŸ“¢ ä»Šæ—¥ç„¡ç¬¦åˆæ–°è")

if __name__ == "__main__":
    main()


