import feedparser
import requests
import re
import time
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs
from bs4 import BeautifulSoup
from summa import summarizer

LINE_ACCESS_TOKEN = 'ä½ çš„ LINE Access Token'
EXCLUDED_KEYWORDS = ['å¾µæ‰', 'æ‹›è˜', 'è·ç¼º', 'å…¼è·', 'é–€å¸‚', 'åŸ¹è¨“', 'ç²—å·¥', 'æŒ‰æ‘©']
CATEGORY_KEYWORDS = {
    'æ–°å…‰é‡‘æ§': ['æ–°å…‰é‡‘æ§', 'æ–°å…‰äººå£½', 'æ–°å£½', 'å³æ±é€²'],
    'å°æ–°é‡‘æ§': ['å°æ–°é‡‘æ§', 'å°æ–°äººå£½', 'å°æ–°å£½', 'å³æ±äº®'],
    'é‡‘æ§': ['é‡‘æ§', 'é‡‘èæ§è‚¡'],
    'ä¿éšª': ['å£½éšª', 'å¥åº·éšª', 'æ„å¤–éšª', 'äººå£½'],
}
CATEGORY_ORDER = ['æ–°å…‰é‡‘æ§', 'å°æ–°é‡‘æ§', 'é‡‘æ§', 'ä¿éšª', 'å…¶ä»–']

def is_taiwan_news(source_name, link):
    # æš«æ™‚å…è¨±æ‰€æœ‰æ–°èä¾†æºï¼Œæ–¹ä¾¿ debug
    return True

def shorten_url(url):
    try:
        resp = requests.post("https://cleanuri.com/api/v1/shorten", data={"url": url}, timeout=10)
        if resp.ok:
            return resp.json().get("result_url")
    except:
        pass
    return url

def resolve_google_news_url(url):
    if 'news.google.com/rss/articles/' in url:
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            resp = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(resp.text, 'html.parser')
            og_url = soup.find('meta', property='og:url')
            if og_url:
                return og_url['content']
        except:
            pass
    return url

def fetch_news(feed_urls):
    all_news = []
    today = datetime.now()
    yesterday = today - timedelta(days=1)
    processed_links = set()

    for url in feed_urls:
        print(f"âœ… ä¾†æº: {url}", end=' ')
        resp = requests.get(url)
        print(f"å›æ‡‰ç‹€æ…‹ï¼š{resp.status_code}")
        d = feedparser.parse(resp.text)
        print(f"âœ… å¾ {url} æŠ“åˆ° {len(d.entries)} ç­†æ–°è")

        for entry in d.entries:
            title = entry.title
            link = entry.link
            source_name = entry.get('source', {}).get('title', '')
            pub_date = entry.get('published', '') or entry.get('pubDate', '')
            try:
                pub_datetime = datetime(*entry.published_parsed[:6])
            except:
                pub_datetime = today  # é è¨­ä»Šå¤©

            print(f"ğŸ“° æ¨™é¡Œ: {title}")
            print(f"    ä¾†æº: {source_name}")
            print(f"    ç™¼å¸ƒæ™‚é–“: {pub_datetime}")

            # æª¢æŸ¥ä¾†æº
            if not is_taiwan_news(source_name, link):
                print(f"    ğŸ›‘ è·³éï¼šéå°ç£ä¾†æº ({source_name})")
                continue

            # æª¢æŸ¥é»‘åå–®é—œéµå­—
            if any(bad_kw in title for bad_kw in EXCLUDED_KEYWORDS):
                print(f"    ğŸ›‘ è·³éï¼šåŒ…å«æ’é™¤é—œéµå­—")
                continue

            # æª¢æŸ¥æ™‚é–“
            if pub_datetime < yesterday:
                print(f"    ğŸ›‘ è·³éï¼šéè¿‘24å°æ™‚")
                continue

            # è§£æåŸå§‹é€£çµ
            original_url = resolve_google_news_url(link)
            if original_url in processed_links:
                print(f"    ğŸ›‘ è·³éï¼šé‡è¤‡æ–°è")
                continue
            processed_links.add(original_url)

            # æŠ“å–åŸæ–‡æ‘˜è¦
            try:
                resp = requests.get(original_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
                soup = BeautifulSoup(resp.text, 'html.parser')
                paragraphs = [p.get_text() for p in soup.find_all('p')]
                full_text = '\n'.join(paragraphs)
                summary = summarizer.summarize(full_text, words=100)
                if not summary:
                    raise ValueError("ç©ºæ‘˜è¦")
                summary = re.sub(r'\s+', ' ', summary.strip())
                short_url = shorten_url(original_url).replace("https://", "line://")  # é˜²æ­¢ preview
                all_news.append((title, summary, short_url))
                print(f"    âœ… åŠ å…¥ï¼š{title}")
            except Exception as e:
                print(f"    âš ï¸ æŠ“å–/æ‘˜è¦å¤±æ•—: {e}")

    return all_news

def classify_news(news_list):
    categories = {cat: [] for cat in CATEGORY_ORDER}
    for title, summary, url in news_list:
        assigned = False
        for cat, keywords in CATEGORY_KEYWORDS.items():
            if any(kw in title for kw in keywords):
                categories[cat].append((title, summary, url))
                assigned = True
                break
        if not assigned:
            categories['å…¶ä»–'].append((title, summary, url))
    return categories

def format_message(categories):
    date_str = datetime.now().strftime("%Y-%m-%d")
    message_parts = []
    for cat in CATEGORY_ORDER:
        items = categories[cat]
        if not items:
            continue
        header = f"ã€{date_str} æ¥­ä¼éƒ¨ ä»Šæ—¥ã€{cat}ã€‘é‡é»æ–°èæ•´ç†ã€‘"
        body = '\n\n'.join([f"ğŸ”¸{title}\nğŸ“{summary}\nğŸ‘‰{url}" for title, summary, url in items])
        message = f"{header}\n\n{body}"
        if len(message) > 4800:
            message = message[:4790] + "\n...(è¨Šæ¯éé•·å·²æˆªæ–·)"
        message_parts.append(message)
    return message_parts

def send_line_message(messages):
    for msg in messages:
        headers = {
            "Authorization": f"Bearer {LINE_ACCESS_TOKEN}",
            "Content-Type": "application/json"
        }
        data = {
            "messages": [{"type": "text", "text": msg}]
        }
        response = requests.post("https://api.line.me/v2/bot/message/broadcast", headers=headers, json=data)
        print(f"ğŸ“¤ ç™¼é€çµæœï¼š{response.status_code} - {response.text}")

if __name__ == "__main__":
    print(f"âœ… Access Token å‰ 10 ç¢¼ï¼š {LINE_ACCESS_TOKEN[:10]}")
    RSS_FEEDS = [
        "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å£½éšª+OR+é‡‘æ§+OR+äººå£½+OR+æ–°å£½+OR+å°æ–°å£½+OR+å³æ±é€²+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+æ–°å£½+OR+å³æ±é€²&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å°æ–°å£½+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=å£½éšª+OR+å¥åº·éšª+OR+æ„å¤–éšª+OR+äººå£½&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
        "https://news.google.com/rss/search?q=é‡‘æ§+OR+é‡‘èæ§è‚¡&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    ]
    news = fetch_news(RSS_FEEDS)
    if not news:
        print("âš ï¸ ç„¡ç¬¦åˆæ¢ä»¶çš„æ–°èï¼Œä¸ç™¼é€ã€‚")
    else:
        categories = classify_news(news)
        messages = format_message(categories)
        send_line_message(messages)
