import os
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import email.utils
from urllib.parse import quote
import requests
from bs4 import BeautifulSoup
import jieba
from summa import summarizer

# å¾ç’°å¢ƒè®Šæ•¸å–å¾— Access Tokenï¼ˆç¢ºä¿åç¨±èˆ‡ GitHub Actions è¨­å®šä¸€è‡´ï¼‰
ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')
if ACCESS_TOKEN:
    print("âœ… Access Token å‰ 10 ç¢¼ï¼š", ACCESS_TOKEN[:10])
else:
    print("âš ï¸ Access Token æœªè¨­å®šï¼Œç„¡æ³•ç™¼é€è¨Šæ¯")

TW_TZ = timezone(timedelta(hours=8))
now = datetime.now(TW_TZ)
today = now.date()

RSS_URLS = [
    "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å£½éšª+OR+é‡‘æ§+OR+äººå£½+OR+æ–°å£½+OR+å°æ–°å£½+OR+å³æ±é€²+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
]

KEYWORDS_ORDER = [
    "æ–°å…‰äººå£½",
    "æ–°å…‰é‡‘æ§",
    "å°æ–°é‡‘æ§",
    "å°æ–°äººå£½",
    "é‡‘æ§",
    "äººå£½",
    "å£½éšª",
    "å¥åº·éšª",
    "æ„å¤–éšª",
]

EXCLUDED_KEYWORDS = ['ä¿éšªå¥—', 'é¿å­•å¥—', 'ä¿éšªå¥—ä½¿ç”¨', 'å¤ªé™½äººå£½', 'å¤§è¥¿éƒ¨äººå£½', 'ç¾åœ‹æµ·å²¸ä¿éšª']

def shorten_url(long_url):
    try:
        encoded_url = quote(long_url, safe='')
        api_url = f"http://tinyurl.com/api-create.php?url={encoded_url}"
        res = requests.get(api_url, timeout=5)
        if res.status_code == 200:
            return res.text.strip()
    except Exception as e:
        print("âš ï¸ çŸ­ç¶²å€å¤±æ•—ï¼š", e)
    return long_url

def is_taiwan_news(source_name, link):
    taiwan_sources = [
        'å·¥å•†æ™‚å ±', 'ä¸­åœ‹æ™‚å ±', 'ç¶“æ¿Ÿæ—¥å ±', 'ä¸‰ç«‹æ–°èç¶²', 'è‡ªç”±æ™‚å ±', 'è¯åˆæ–°èç¶²',
        'é¡é€±åˆŠ', 'å°ç£é›…è™', 'é‰…äº¨ç¶²', 'ä¸­æ™‚æ–°èç¶²', 'Ettodayæ–°èé›²',
        'å¤©ä¸‹é›œèªŒ', 'å¥‡æ‘©æ–°è', 'ã€Šç¾ä»£ä¿éšªã€‹é›œèªŒ', 'é è¦‹é›œèªŒ'
    ]
    if any(src in source_name for src in taiwan_sources) and "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±" not in source_name:
        return True
    if '.tw' in link:
        return True
    return False

def fetch_article_content(url):
    try:
        res = requests.get(url, timeout=10)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, 'html.parser')
        selectors = [
            'article',
            'div.article-content',
            'div#article-body',
            'div#content',
            'div[itemprop="articleBody"]',
            'div.story-content',
            'section.article',
        ]
        text = ""
        for sel in selectors:
            content = soup.select_one(sel)
            if content:
                text = content.get_text(separator='\n').strip()
                if len(text) > 200:
                    break
        if not text:
            text = soup.get_text(separator='\n').strip()
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        cleaned_text = "\n".join(lines)
        return cleaned_text
    except Exception as e:
        print(f"âš ï¸ æŠ“å–æ–‡ç« å…§å®¹å¤±æ•—: {e}")
        return ""

def summarize_text(text, max_words=100):
    try:
        return summarizer.summarize(text, words=max_words)
    except Exception as e:
        print(f"âš ï¸ æ‘˜è¦å¤±æ•—: {e}")
        return text[:max_words]

def fetch_news():
    news_list = []
    print("âœ… å–å¾— RSS ä¸­...")
    for rss_url in RSS_URLS:
        try:
            res = requests.get(rss_url, timeout=10)
            if res.status_code != 200:
                print(f"âš ï¸ å–å¾— RSS å¤±æ•—: {rss_url}")
                continue
            root = ET.fromstring(res.content)
            items = root.findall(".//item")
            print(f"âœ… RSS: {rss_url} å…± {len(items)} ç­†")
            for item in items:
                title_elem = item.find('title')
                link_elem = item.find('link')
                pubDate_elem = item.find('pubDate')
                source_elem = item.find('source')
                if not title_elem or not link_elem or not pubDate_elem:
                    continue
                title = title_elem.text.strip()
                link = link_elem.text.strip()
                pubDate_str = pubDate_elem.text.strip()
                source_name = source_elem.text.strip() if source_elem is not None else "æœªæ¨™ç¤º"
                if not title or any(bad_kw in title for bad_kw in EXCLUDED_KEYWORDS):
                    continue
                pub_datetime = email.utils.parsedate_to_datetime(pubDate_str).astimezone(TW_TZ)
                if now - pub_datetime > timedelta(hours=24):
                    continue
                if not is_taiwan_news(source_name, link):
                    continue
                # æŠ“æ–‡ç« æ‘˜è¦
                full_text = fetch_article_content(link)
                summary = summarize_text(full_text, max_words=100)
                if not summary:
                    summary = title
                short_link = shorten_url(link)

                # æŒ‰é—œéµå­—æ’åº
                keyword_found = None
                for kw in KEYWORDS_ORDER:
                    if kw in title:
                        keyword_found = kw
                        break
                if not keyword_found:
                    keyword_found = "å…¶ä»–"

                news_list.append((KEYWORDS_ORDER.index(keyword_found) if keyword_found in KEYWORDS_ORDER else 999,
                                  f"ğŸ“° {title}\nâœï¸ {summary}\nğŸ”— {short_link}"))
        except Exception as e:
            print("âš ï¸ è®€å– RSS ç™¼ç”ŸéŒ¯èª¤:", e)
            continue

    # ä¾é—œéµå­—æ’åº
    news_list.sort(key=lambda x: x[0])
    return [n[1] for n in news_list]

def send_message(news_list):
    if not ACCESS_TOKEN:
        print("âš ï¸ ACCESS_TOKEN æœªè¨­å®šï¼Œç„¡æ³•ç™¼é€è¨Šæ¯")
        return
    if not news_list:
        print("âš ï¸ ç„¡æœ‰æ•ˆæ‘˜è¦å¯ç™¼é€")
        return

    max_length = 4000
    message = f"ã€{today} æ¥­ä¼éƒ¨ ä»Šæ—¥é‡é»æ–°èæ•´ç†ã€‘ å…±{len(news_list)}å‰‡æ–°è\n\n"
    message += "\n\n".join(news_list)
    if len(message) > max_length:
        message = message[:max_length]
        print(f"âš ï¸ è¶…å‡º {max_length} å­—å…ƒé™åˆ¶ï¼Œè¨Šæ¯å·²æˆªæ–·")

    url = 'https://api.line.me/v2/bot/message/broadcast'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {ACCESS_TOKEN}'
    }
    data = {
        "messages": [{
            "type": "text",
            "text": message
        }]
    }
    print(f"ğŸ“¤ ç™¼é€è¨Šæ¯ç¸½é•·ï¼š{len(message)} å­—å…ƒ")
    res = requests.post(url, headers=headers, json=data)
    print(f"ğŸ“¤ LINE å›å‚³ç‹€æ…‹ç¢¼ï¼š{res.status_code}")
    print("ğŸ“¤ LINE å›å‚³å…§å®¹ï¼š", res.text)

if __name__ == "__main__":
    news = fetch_news()
    print(f"âœ… éæ¿¾å¾Œçš„å°ç£æ–°èå…± {len(news)} å‰‡")
    send_message(news)



