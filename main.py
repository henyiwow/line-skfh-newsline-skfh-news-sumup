import os
import feedparser
from datetime import datetime, timedelta, timezone
from urllib.parse import quote
import requests
from bs4 import BeautifulSoup
from summa import summarizer

ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')
print("âœ… Access Token å‰ 10 ç¢¼ï¼š", ACCESS_TOKEN[:10] if ACCESS_TOKEN else "æœªè¨­å®š")

TW_TZ = timezone(timedelta(hours=8))
now = datetime.now(TW_TZ)
today = now.date()

RSS_URLS = [
    "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å£½éšª+OR+é‡‘æ§+OR+äººå£½+OR+æ–°å£½+OR+å°æ–°å£½+OR+å³æ±é€²+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=æ–°å…‰é‡‘æ§+OR+æ–°å…‰äººå£½+OR+æ–°å£½+OR+å³æ±é€²&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å°æ–°é‡‘æ§+OR+å°æ–°äººå£½+OR+å°æ–°å£½+OR+å³æ±äº®&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å£½éšª+OR+å¥åº·éšª+OR+æ„å¤–éšª+OR+äººå£½&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=é‡‘æ§+OR+é‡‘èæ§è‚¡&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
]

EXCLUDED_KEYWORDS = ['ä¿éšªå¥—', 'é¿å­•å¥—', 'ä¿éšªå¥—ä½¿ç”¨', 'å¤ªé™½äººå£½', 'å¤§è¥¿éƒ¨äººå£½', 'ç¾åœ‹æµ·å²¸ä¿éšª']
TAIWAN_SOURCES = [
    'å·¥å•†æ™‚å ±', 'ä¸­åœ‹æ™‚å ±', 'ç¶“æ¿Ÿæ—¥å ±', 'ä¸‰ç«‹æ–°èç¶²', 'è‡ªç”±æ™‚å ±', 'è¯åˆæ–°èç¶²',
    'é¡é€±åˆŠ', 'å°ç£é›…è™', 'é‰…äº¨ç¶²', 'ä¸­æ™‚æ–°èç¶²', 'Ettodayæ–°èé›²',
    'å¤©ä¸‹é›œèªŒ', 'å¥‡æ‘©æ–°è', 'ã€Šç¾ä»£ä¿éšªã€‹é›œèªŒ', 'é è¦‹é›œèªŒ'
]

def shorten_url(long_url):
    try:
        encoded_url = quote(long_url, safe='')
        res = requests.get(f"http://tinyurl.com/api-create.php?url={encoded_url}", timeout=5)
        return res.text.strip() if res.status_code == 200 else long_url
    except:
        return long_url

def is_taiwan_news(source, link):
    if any(src in source for src in TAIWAN_SOURCES) and "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±" not in source:
        return True
    if ".tw" in link:
        return True
    return False

def fetch_article_content(url):
    try:
        res = requests.get(url, timeout=10)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, 'html.parser')
        selectors = [
            'article', 'div.article-content', 'div#article-body',
            'div#content', 'div[itemprop="articleBody"]', 'div.story-content',
            'section.article',
        ]
        text = ""
        for sel in selectors:
            content = soup.select_one(sel)
            if content:
                text = content.get_text(separator='\n').strip()
                if len(text) > 200:
                    break
        if not text:
            text = soup.get_text(separator='\n').strip()
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        return "\n".join(lines)
    except:
        return ""

def summarize_text(text, max_words=100):
    try:
        return summarizer.summarize(text, words=max_words)
    except:
        return text[:max_words]

def fetch_all_news():
    results = []
    for url in RSS_URLS:
        feed = feedparser.parse(url)
        print(f"âœ… RSS: {url} å…± {len(feed.entries)} ç­†")
        for entry in feed.entries:
            title = entry.title.strip()
            link = entry.link.strip()
            pub_date = entry.published_parsed
            source = entry.get('source', {}).get('title', 'æœªçŸ¥ä¾†æº')
            if not pub_date:
                continue
            pub_dt = datetime(*pub_date[:6], tzinfo=timezone.utc).astimezone(TW_TZ)
            if now - pub_dt > timedelta(hours=24):
                continue
            if any(bad_kw in title for bad_kw in EXCLUDED_KEYWORDS):
                continue
            if not is_taiwan_news(source, link):
                continue
            short_link = shorten_url(link)
            full_text = fetch_article_content(link)
            summary = summarize_text(full_text, max_words=100) or title
            results.append(
                f"ğŸ“° {title}\nğŸ“Œ ä¾†æºï¼š{source}\nâœï¸ æ‘˜è¦ï¼š{summary}\nğŸ”— {short_link}"
            )
    return results

def send_news_message(news_list):
    if not news_list:
        print("âš ï¸ ç„¡ç¬¦åˆæ¢ä»¶çš„æ–°èï¼Œä¸ç™¼é€ã€‚")
        return
    message = f"ã€{today} æ¥­ä¼éƒ¨ ä»Šæ—¥é‡é»æ–°èæ•´ç†ã€‘ å…±{len(news_list)}å‰‡\n\n"
    message += "\n\n".join(news_list)
    if len(message) > 4000:
        message = message[:4000]
        print("âš ï¸ è¶…å‡º 4000 å­—å…ƒé™åˆ¶ï¼Œè¨Šæ¯å·²æˆªæ–·")
    broadcast_message(message)

def broadcast_message(message):
    if not ACCESS_TOKEN:
        print("âš ï¸ ACCESS_TOKEN æœªè¨­å®šï¼Œç„¡æ³•ç™¼é€è¨Šæ¯")
        return
    url = 'https://api.line.me/v2/bot/message/broadcast'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {ACCESS_TOKEN}'
    }
    data = {
        "messages": [{"type": "text", "text": message}]
    }
    print(f"ğŸ“¤ ç™¼é€è¨Šæ¯å­—æ•¸ï¼š{len(message)}")
    res = requests.post(url, headers=headers, json=data)
    print(f"ğŸ“¤ LINE å›æ‡‰ç‹€æ…‹ç¢¼ï¼š{res.status_code}")
    print(f"ğŸ“¤ LINE å›æ‡‰å…§å®¹ï¼š{res.text}")

if __name__ == "__main__":
    news_items = fetch_all_news()
    send_news_message(news_items)


